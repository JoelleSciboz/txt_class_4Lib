{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification and Legacy Library Metadata\n",
    "\n",
    "This project aims at:\n",
    "\n",
    "* evaluating the potentials and limitations of using legacy library metadata to automatically annotate textual documents;\n",
    "* providing a model to automatically annotate textual documents, that could be used by system and metadata librarians to learn and simulate the learning process.\n",
    "\n",
    "The notebook is organized as follow:\n",
    "    1. Introduction to text classification\n",
    "        1.1 Concepts and terminology\n",
    "        1.2 Classification process\n",
    "        1.3 Learning algorithms\n",
    "    2. Case study:\n",
    "        2.1 Dataset\n",
    "        2.2 Data cleaning and normalization\n",
    "        2.3 Label binarization\n",
    "        2.4 Features engineering\n",
    "        2.5 Text classification pipelines\n",
    "        2.6 Analysis and reports\n",
    "    4. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction to text classification\n",
    "### 1.1. Concepts and definitions\n",
    "\n",
    "**Text mining:** Text mining explores and identifies patterns in unstructured textual data with the aim of extracting structured information and infer knowledge. It relies on methods from the fields of information retrieval, natural language processing, statistics, machine learning and data mining.\n",
    "\n",
    "**Machine Learning**\n",
    "\n",
    "Murphy (2012, p.1) defines machine learning as a “set of methods that can automatically detect patterns in data, and then use the uncovered patterns to predict future data, or to perform other kinds of decision making under uncertainty.” A learning model executes a program that uses training data to optimize a set of parameters. A model can suffer from two different issues. __Underfitting__ occurs when the model makes many training errors. It happens when the hypothesis function is too simple. Adding features or increasing the complexity of the hypothesis help to mitigate this problem. __Overfitting__ occurs when the model fits perfectly the training data, in other words when the proportion of errors is low. Consequently, the model will likely fail to generalize to new examples that are not in the training set. :\n",
    "\n",
    "**Classification**\n",
    "\n",
    "Classification is learning task that attempts to learn a relationship between input __features__ and output __labels__ from a set of examples that are already labeled. In text classification, features are typically words or tokens found in the title or text. Labels are categories assigned to the text, for instance library subject headings. **Binary classification** refers to a learning problem whose goal is to output one label by example out of two possible classes. In **multi-class classification** problems, the aim is to output one label for each example, out of q possible ones, where q > 2. **Multi-label classification** attempt to assign several labels to each text. It is a common problem when dealing with library material as a book, an article is assigned several subject headings. Multi-label classification is a computationally expensive and complex learning problem. The complexity and computational costs increase with the size of the vocabulary used to index the documents.\n",
    "\n",
    "**Vectorization** \n",
    "\n",
    "Vectorization is the process of converting a text into a vectorized representation of its content. Each token (word or group of words) becomes a feature represented by a numberical value that will be used by machine learning algorithms. There several possibilities to build the vectorized representation of the text including:\n",
    "* each feature is represented by a boolean value (0,1), the process of converting features in boolean value is called **binarization**.\n",
    "* each feature is represented by its **frequency** in the text.\n",
    "* each feature is represented by its frequency in the text normalized by the frequency of the features in the overall corpus. A common measure is the **term frequency-inverse document frequency (TF-IDF)**\n",
    "\n",
    "**Document term matrix** \n",
    "\n",
    "The result of vectorization is a document-term-matrix, where one row represent a text and one column a feature. The number indicated is either a binary value (0 is not in text, 1 is in text), or the term frequency in the text, or the tf-itf score. When the measure is a binary value, then the matrix is called binary matrix. **binary matrices** are also used to represent the association of each record with each label in the dataset. For instance in the following example, record 32 is labelled with ABKHAZIA (GEORGIA) and AERIAL BOMBINGS.\n",
    "\n",
    "| 1965  | 1972  |  development |environment|  established |  programme  | united |nations| \n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "|1|0|1|0|1|1|1|1|\n",
    "|0|1|0|1|1|1|1|1|\n",
    "\n",
    "**Label binarization** \n",
    "\n",
    "Label binarization is the process of converting labesl into a **binary matrix** that represents the association of each record with each label in the dataset. For instance in the following example, record 32 is labelled with ABKHAZIA (GEORGIA) and AERIAL BOMBINGS.\n",
    "\n",
    "| record_id  | ABDUCTION  |  ABKHAZIA (GEORGIA) | ABU MUSA | ABYEI (SUDAN) | ADMINISTRATIVE PROCEDURE | AERIAL BOMBINGS |\n",
    "|---|---|---|---|---|---|---|\n",
    "|32|0|1|0|0|0|1|\n",
    "\n",
    "**Feature engeneering** \n",
    "\n",
    "Feature engeneering consists of extracting and selecting the most pertinent features to build a simpler representation of the text, usually a vector representation.  **Feature extraction** applies several pre-processing techniques such as tokenization, stemming, lower case and stop words filtering. **Tokenization** uses white spaces and special characters to split the text into a stream of words. **Lower case and stop words filtering** transform all words in lower case and remove non-significative words such as the, is, a, an. **Stemming** suppresses prefixes and suffixes to reduce many word forms to a common root, which might be but does not always is a valid semantic root. **Feature selection** is the process of selecting the most pertinent tokens to feed the learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Text classification Process\n",
    "\n",
    "    \n",
    "    1. Data pre-processing\n",
    "        1.1 Data acquisition\n",
    "        1.2 Data cleaning and normalization\n",
    "    2. Training\n",
    "        2.1 Label extraction and binarization\n",
    "        2.2 Features extraction and selection\n",
    "        2.3 Model selection\n",
    "        2.4 Testing and cross-validation\n",
    "        2.5 Analysis\n",
    "    3. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Learning Algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
