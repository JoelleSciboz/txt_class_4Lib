{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A - 1 - Data Acquistion from MARC XML records\n",
    "\n",
    "## Description\n",
    "**Process aim:**\n",
    "This process aims at acquiring a MARC XML representing a collection of records, fetching and extracting full text of the ressources described, and saving all information in a CSV file.  The majority of the MARC records should contain at least one URL to locate the full text of the resources in PDF.\n",
    "\n",
    "**Input:** a MARC XML file saved in data/acquisition\n",
    "\n",
    "**Sub-processes**:\n",
    "1. Import and Parse MARC XML\n",
    "2. Convert MARC XML to a Dataframe\n",
    "2. Get, save and exctract full text from PDF files\n",
    "\n",
    "**Output:** a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Libraries required in this section\n",
    "# lxml: used to parse and extract data from an XML file\n",
    "from lxml import etree, objectify\n",
    "# Pandas: used to structure metadata as table for better visualization and manipulation\n",
    "import pandas as pd\n",
    "# Requests: requests is used to download files via http\n",
    "import requests\n",
    "# PyPDF2: library to read and manipulate pdf files\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import and Parse MARC XML\n",
    "* **Input**: MARC XML file\n",
    "* **Output**: Collection of XML records extracted from the file\n",
    "* **Customization:** None, the code can be run, without customization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_prefixes(xml_root):\n",
    "    '''\n",
    "    This function takes root as argument and clean it from any prefix, before returning it.\n",
    "    '''\n",
    "    for e in xml_root.getiterator():\n",
    "        if hasattr(e.tag, 'find'):\n",
    "            i = e.tag.find('}')\n",
    "            if i >= 0:\n",
    "                e.tag = e.tag[i+1:]\n",
    "    return xml_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a variable with parameters on how the parser should behave.\n",
    "parser = etree.XMLParser(encoding='utf-8')\n",
    "# Import xml file as an xml etree\n",
    "tree = etree.parse('data/acquisition/undl_marc.xml',parser)\n",
    "# Remove prefixes in XML elements\n",
    "root = clean_prefixes(tree.getroot())\n",
    "# Get all <records> element in XML MARC\n",
    "xml_records = root.findall(\"record\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Convert MARC XML to a Dataframe\n",
    "* **Aim:** The aim of this sub-process is to extract the relevant values from the XML records, organize and store them in data structure for furhter manipulation and processing.\n",
    "* **Input**: A collection of MARC XML records\n",
    "* **Steps**:\n",
    "    * Extract and get metadata values\n",
    "    * Create, check and reshape the dataframe of Metadata\n",
    "* **Output:** A dataframe containing all relevant metadata\n",
    "* **Customization:** remove and add desired metadata fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract and get metadata values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_medatata()\n",
    "This function returns a dictionary for each record in the MARC XML. The structure of the dictionary is:\n",
    "<pre><code>\n",
    "{\n",
    "field1: 'value'\n",
    "field2: ['value1', 'value2']\n",
    "}\n",
    "</code></pre>\n",
    "\n",
    "* field1 represent a field with only one value, field2 a field with multiple values.\n",
    "\n",
    "***Customization***\n",
    "\n",
    "You can remove unwanted fields by adding a **#** in front of the relevant line. To add a new field:\n",
    "<pre><code>\"variable_name\": get_data(r,\"datafield[@tag='MARC_tag'][@ind1='indicator']/subfield[@code='MARC_subfield']\"),</code></pre>\n",
    "\n",
    "For instance:\n",
    "* variable_name: report;\n",
    "* a MARC_field: @tag='993'\n",
    "* an indicator: @ind1='3' (optional)\n",
    "* a MARC_subfield: subfield[@code='a']\") (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata(r):\n",
    "    \"\"\"\n",
    "    Takes a MARC XML record. For each metadata, the function will call annother get_element, which will return\n",
    "    'None', a string (one value found), or a list of string (multiple values found)\n",
    "    \"\"\"\n",
    "    # Each record will be a dictionary and include\n",
    "    metadata =  {\n",
    "        # Standard MARC fields\n",
    "        \"record_id\": get_md_value(r,\"controlfield[@tag='001']\"),\n",
    "        # The title field concatenate 3 subfields (a, b, c)\n",
    "        \"title\": get_md_value(\n",
    "            r,\"datafield[@tag='245']/subfield[@code='a']\") + \" \" + get_md_value(\n",
    "            r,\"datafield[@tag='245']/subfield[@code='b']\") + \" \" + get_md_value(\n",
    "            r,\"datafield[@tag='245']/subfield[@code='c']\"),\n",
    "        \"topics\": get_md_value(r,\"datafield[@tag='650']/subfield[@code='a']\"),\n",
    "        \"geographic_terms\": get_md_value(r,\"datafield[@tag='651']/subfield[@code='a']\"),\n",
    "        \"corporates\": get_md_value(r,\"datafield[@tag='610']/subfield[@code='a']\"),      \n",
    "        # Local MARC fields in used for UN documents\n",
    "        \"symbol\": get_md_value(r,\"datafield[@tag='191']/subfield[@code='a']\"),\n",
    "        \"body\": get_md_value(r,\"datafield[@tag='191']/subfield[@code='b']\"),\n",
    "        \"session\": get_md_value(r,\"datafield[@tag='191']/subfield[@code='c']\"),\n",
    "    }\n",
    "    # Call the function get_files_info. This function return annother dictionary{description: url}\n",
    "    files_info = get_files_md(r)\n",
    "    # Merge the initial metadata dictionary with the dictionary containing the files information\n",
    "    metadata = {**metadata,**files_info}\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_files_md()\n",
    "This function is used by get_metadata to get information recorded in MARC 856, the description of the file and its url. Based on a metadata record it returns a the following data structure:\n",
    "<pre><code>\n",
    "{\n",
    "    description: url\n",
    "}\n",
    "</code></pre>\n",
    "\n",
    "By default, it returns, the values in subfield y (description) and subfield u (url) MARC fields 856 with the first indicator set to 4, which inidicate that the resourcs is accessible via http.\n",
    "\n",
    "***Customization***\n",
    "\n",
    "The subfield used for the description can be modified. For instance, if you would rather have the file extension, change the reference to subfield y to subfield q.\n",
    "<pre><code>\n",
    "description = get_md_value(item, \"subfield[@code='y']\")\n",
    "</code></pre>\n",
    "to \n",
    "<pre><code>\n",
    "description = get_md_value(item, \"subfield[@code='q']\")\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_md(r):\n",
    "    '''\n",
    "    This function get the URL from the xml records as well as the description. It forms a dictionary,\n",
    "    where the description is the key and the url the value.\n",
    "    '''\n",
    "    files = {}\n",
    "    MARC_856 = r.findall(\"datafield[@tag='856'][@ind1='4']\")\n",
    "    if len(MARC_856) > 0:\n",
    "        for item in MARC_856:\n",
    "            description = \"url-\" + get_md_value(item, \"subfield[@code='y']\")\n",
    "            # Deals with some variation of encoding in the name of languages (Specific to UN)\n",
    "            description = description.replace('ñ','ñ').replace('ç','ç')\n",
    "            url = get_md_value(item, \"subfield[@code='u']\")\n",
    "            files[description] = url\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get_md_value()\n",
    "\n",
    "This function is used by the two preceedings functions (get_metadata and get_files_md). It process XML record and get the relevant MARC field or subfield identified by the query. It returns a string, or a list of strings. It assumes that the MARC XML element queries does not have children. If the element has a children or if it does not exists, then the function will return an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_md_value(marc_xml_record,query):\n",
    "    \"\"\"\n",
    "    Takes 2 arguments: a MARC XML record and a query (XPath) that identifies an XML element. \n",
    "    Queries the record to identify the targeted element. If no element is found return 'None', if one element is found returns a string, if more than one\n",
    "    elements are found returns a list of strings.\n",
    "    \"\"\"\n",
    "    # Parse the record to get all matchin gelements\n",
    "    xml_element = marc_xml_record.findall(query)\n",
    "    # Process xml_element according to its length to return either a string, or a list of strings.\n",
    "    if len(xml_element)>1:\n",
    "        values = []\n",
    "        i = 0\n",
    "        for item in xml_element:\n",
    "            element = values.append(xml_element[i].text)\n",
    "            i +=1\n",
    "        return values # multiple values, retunrns a list of strings\n",
    "    elif len(xml_element) == 1:\n",
    "        return xml_element[0].text # one value, retunrns a string\n",
    "    else:\n",
    "        return \"\" # no value, returns an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a list of dictionaries containing the metadata specified in the get_metadata function\n",
    "dictionary_records = [get_metadata(r) for r in xml_records]   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create, check and reshape the metadta dataframe\n",
    "From the dictionary, a dataframe can be created. These are table-like structures that ease data manipulation and extraction. Some basic information on the dataset can be obtained using:\n",
    "* md_dataset.info(): print information about the datset, for instance:\n",
    "    ** number of entries (rows)\n",
    "    ** number of non-null value by columns\n",
    "* md_dataset.columns: name of columns\n",
    "* md_dataset.head(): print the five first row of the table, including the headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 200 entries, 703243 to 702423\n",
      "Data columns (total 15 columns):\n",
      "body                200 non-null object\n",
      "corporates          200 non-null object\n",
      "geographic_terms    200 non-null object\n",
      "session             200 non-null object\n",
      "symbol              200 non-null object\n",
      "title               200 non-null object\n",
      "topics              200 non-null object\n",
      "url-                1 non-null object\n",
      "url-English         198 non-null object\n",
      "url-Español        195 non-null object\n",
      "url-Français       198 non-null object\n",
      "url-Other           1 non-null object\n",
      "url-Русский         197 non-null object\n",
      "url-العربية         194 non-null object\n",
      "url-中文              196 non-null object\n",
      "dtypes: object(15)\n",
      "memory usage: 25.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Create a data frame\n",
    "md_dataset = (pd.DataFrame(dictionary_records)\n",
    "              .set_index('record_id'))  \n",
    "# Get dataframe information\n",
    "md_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>corporates</th>\n",
       "      <th>geographic_terms</th>\n",
       "      <th>session</th>\n",
       "      <th>symbol</th>\n",
       "      <th>title</th>\n",
       "      <th>topics</th>\n",
       "      <th>url-</th>\n",
       "      <th>url-English</th>\n",
       "      <th>url-Español</th>\n",
       "      <th>url-Français</th>\n",
       "      <th>url-Other</th>\n",
       "      <th>url-Русский</th>\n",
       "      <th>url-العربية</th>\n",
       "      <th>url-中文</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>record_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>703243</th>\n",
       "      <td>A/</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>65</td>\n",
       "      <td>A/65/PV.71</td>\n",
       "      <td>General Assembly official records, 65th sessio...</td>\n",
       "      <td>[SOCIAL DEVELOPMENT, AGEING PERSONS, DEMOGRAPH...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://digitallibrary.un.org/record/703243/fil...</td>\n",
       "      <td>http://digitallibrary.un.org/record/703243/fil...</td>\n",
       "      <td>http://digitallibrary.un.org/record/703243/fil...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://digitallibrary.un.org/record/703243/fil...</td>\n",
       "      <td>http://digitallibrary.un.org/record/703243/fil...</td>\n",
       "      <td>http://digitallibrary.un.org/record/703243/fil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703245</th>\n",
       "      <td>A/</td>\n",
       "      <td>[UN. Peacebuilding Commission. Organizational ...</td>\n",
       "      <td></td>\n",
       "      <td>65</td>\n",
       "      <td>A/65/PV.72</td>\n",
       "      <td>General Assembly official records, 65th sessio...</td>\n",
       "      <td>[POPULATION PROGRAMMES, CHEMICAL WEAPONS, COOP...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://digitallibrary.un.org/record/703245/fil...</td>\n",
       "      <td>http://digitallibrary.un.org/record/703245/fil...</td>\n",
       "      <td>http://digitallibrary.un.org/record/703245/fil...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>http://digitallibrary.un.org/record/703245/fil...</td>\n",
       "      <td>http://digitallibrary.un.org/record/703245/fil...</td>\n",
       "      <td>http://digitallibrary.un.org/record/703245/fil...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          body                                         corporates  \\\n",
       "record_id                                                           \n",
       "703243      A/                                                      \n",
       "703245      A/  [UN. Peacebuilding Commission. Organizational ...   \n",
       "\n",
       "          geographic_terms session      symbol  \\\n",
       "record_id                                        \n",
       "703243                          65  A/65/PV.71   \n",
       "703245                          65  A/65/PV.72   \n",
       "\n",
       "                                                       title  \\\n",
       "record_id                                                      \n",
       "703243     General Assembly official records, 65th sessio...   \n",
       "703245     General Assembly official records, 65th sessio...   \n",
       "\n",
       "                                                      topics url-  \\\n",
       "record_id                                                           \n",
       "703243     [SOCIAL DEVELOPMENT, AGEING PERSONS, DEMOGRAPH...  NaN   \n",
       "703245     [POPULATION PROGRAMMES, CHEMICAL WEAPONS, COOP...  NaN   \n",
       "\n",
       "                                                 url-English  \\\n",
       "record_id                                                      \n",
       "703243     http://digitallibrary.un.org/record/703243/fil...   \n",
       "703245     http://digitallibrary.un.org/record/703245/fil...   \n",
       "\n",
       "                                                url-Español  \\\n",
       "record_id                                                      \n",
       "703243     http://digitallibrary.un.org/record/703243/fil...   \n",
       "703245     http://digitallibrary.un.org/record/703245/fil...   \n",
       "\n",
       "                                               url-Français url-Other  \\\n",
       "record_id                                                                \n",
       "703243     http://digitallibrary.un.org/record/703243/fil...       NaN   \n",
       "703245     http://digitallibrary.un.org/record/703245/fil...       NaN   \n",
       "\n",
       "                                                 url-Русский  \\\n",
       "record_id                                                      \n",
       "703243     http://digitallibrary.un.org/record/703243/fil...   \n",
       "703245     http://digitallibrary.un.org/record/703245/fil...   \n",
       "\n",
       "                                                 url-العربية  \\\n",
       "record_id                                                      \n",
       "703243     http://digitallibrary.un.org/record/703243/fil...   \n",
       "703245     http://digitallibrary.un.org/record/703245/fil...   \n",
       "\n",
       "                                                      url-中文  \n",
       "record_id                                                     \n",
       "703243     http://digitallibrary.un.org/record/703243/fil...  \n",
       "703245     http://digitallibrary.un.org/record/703245/fil...  "
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print a limited number of rows, in this case 2\n",
    "md_dataset.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['body', 'corporates', 'geographic_terms', 'session', 'symbol', 'title',\n",
       "       'topics', 'url-', 'url-English', 'url-Español', 'url-Français',\n",
       "       'url-Other', 'url-Русский', 'url-العربية', 'url-中文'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the columns names\n",
    "md_dataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing and renaming columns\n",
    "* Using .drop() we remove values with limited information contained in columns 'url-' and 'url-Other'.\n",
    "* Using .rename() we rename the url columns to remove any special character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unwanted colums and shorten some columns name\n",
    "md_dataset = (md_dataset\n",
    "              .drop(['url-','url-Other'], axis=1) # delete columns 'url-' and 'url-Other'\n",
    "              .rename(columns={'url-English':'url-en','url-Español':'url-es','url-Français':'url-fr',\n",
    "                               'url-Русский': 'url-ru','url-العربية':'url-ar','url-中文': 'url-zh'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 200 entries, 703243 to 702423\n",
      "Data columns (total 13 columns):\n",
      "body                200 non-null object\n",
      "corporates          200 non-null object\n",
      "geographic_terms    200 non-null object\n",
      "session             200 non-null object\n",
      "symbol              200 non-null object\n",
      "title               200 non-null object\n",
      "topics              200 non-null object\n",
      "url-en              198 non-null object\n",
      "url-es              195 non-null object\n",
      "url-fr              198 non-null object\n",
      "url-ru              197 non-null object\n",
      "url-ar              194 non-null object\n",
      "url-zh              196 non-null object\n",
      "dtypes: object(13)\n",
      "memory usage: 21.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Check the dataframe information to ensure it was correctly process\n",
    "md_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get, save and exctract full text from PDF files\n",
    "* **Aim**: This process aims at adding a field containing the full text of the resources to the metadata dataframe.\n",
    "* **Input**: metadata dataframe\n",
    "* **Steps**: \n",
    "    * Get and save PDF files\n",
    "    * Extract full text from PDF files\n",
    "    * Add full texts to the metadata dataframe and save the output in data/preprocessing\n",
    "* **Output**: metadata dataframe with full text, CSV file\n",
    "\n",
    "### Get and save PDF files\n",
    "We need the full text of the resources described in the MARC XML that will be used later to infer some metadata. In this case we will focus on English texts only.\n",
    "\n",
    "For this step start by creating a list containing for each record that has an English url, the record id, and the url. We then use the function save_files() to get the files using the url and save them in the the folder data/acquisition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_files(files_list, save_path, file_extension):\n",
    "    '''\n",
    "    Takes a list of of lists (record_id and url), the path of the location where the files\n",
    "    will be saved, and the extension of the file type. Get the files through htpp requests\n",
    "    and save them. Returns a list of record_id, corresponding to files that could not be \n",
    "    saved.\n",
    "    '''\n",
    "    not_saved = []\n",
    "    for item in files_list:\n",
    "        save_as = save_path + item[0] + file_extension\n",
    "        file_url = item[1]\n",
    "        response = requests.get(file_url)\n",
    "        if response.status_code == requests.codes.ok:\n",
    "            with open(save_as, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "        else:\n",
    "            not_saved.append(item)\n",
    "    return not_saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# From md_dataset, we create a list of lists with 2 values. The record_id and the url of the English file\n",
    "en_list = (md_dataset.reset_index()[['record_id', 'url-en']]\n",
    "           .dropna() # filter out if non values\n",
    "           .values.tolist())\n",
    "# Get the files and same them in pdf\n",
    "error = save_files(en_list, 'data/acquisition/', '.pdf')\n",
    "# Check if errors\n",
    "print(len(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract full text from PDF files\n",
    "Using the same files list we then use the convert_to_pdf_function, to get the content of the PDFs, convert it to a string of text, and store this as a third column in the initial list. Note that if a page cannot be processed, then it will be skipped altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_text(files_list, path):\n",
    "    '''\n",
    "    Takes a list of list, and a path to files in pdf. Read each files and convert to text.\n",
    "    Append the resulting texts to the initial list and return the list.\n",
    "    '''\n",
    "    new_list = []\n",
    "    for file in files_list:\n",
    "        file_path = path + file[0] + '.pdf'\n",
    "        pdf_file = open(file_path, 'rb')\n",
    "        pdf_reader = PyPDF2.PdfFileReader(pdf_file)\n",
    "        full_text = \"\"\n",
    "        for page in range(0,pdf_reader.getNumPages()):\n",
    "            # Try to convert page by page\n",
    "            try:\n",
    "                page = pdf_reader.getPage(page)\n",
    "                page_text = page.extractText()\n",
    "                page_text = page_text.replace('\\n','')\n",
    "                full_text += page_text\n",
    "            except: # If one page cannot be processed, pass and continue with the next one\n",
    "                pass\n",
    "        file.append(full_text)    \n",
    "        new_list.append(file)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PdfReadWarning: Xref table not zero-indexed. ID numbers for objects will be corrected. [pdf.py:1736]\n"
     ]
    }
   ],
   "source": [
    "# Add a column with the full text of the pdf\n",
    "en_list = convert_pdf_to_text(en_list,'data/acquisition/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add full texts to the metadata dataframe and save the output in data/preprocessing\n",
    "To finish, we create a new dataframe with only the record_id and the full text. As they have the same index, the record-id, we can join it to the metadata dataframe and easily save the result as a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 200 entries, 703243 to 702423\n",
      "Data columns (total 14 columns):\n",
      "body                200 non-null object\n",
      "corporates          200 non-null object\n",
      "geographic_terms    200 non-null object\n",
      "session             200 non-null object\n",
      "symbol              200 non-null object\n",
      "title               200 non-null object\n",
      "topics              200 non-null object\n",
      "url-en              198 non-null object\n",
      "url-es              195 non-null object\n",
      "url-fr              198 non-null object\n",
      "url-ru              197 non-null object\n",
      "url-ar              194 non-null object\n",
      "url-zh              196 non-null object\n",
      "text                198 non-null object\n",
      "dtypes: object(14)\n",
      "memory usage: 33.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Create a new dataframe withg the record_id and the full text\n",
    "full_text = (pd.DataFrame(en_list, columns=['record_id','url','text'])\n",
    "             .drop('url', axis=1)\n",
    "             .set_index('record_id')\n",
    "            )\n",
    "# Join the result to the metadta dataset\n",
    "md_dataset = md_dataset.join(full_text)\n",
    "# Check the result\n",
    "md_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the content of the dataset in data/pre-processing/\n",
    "md_dataset.to_csv('data/pre-processing/dataset_from_MARC.csv')\n",
    "# In case you prefer json\n",
    "md_dataset.to_json('data/pre-processing/dataset_from_MARC.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
