{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B - 2 - Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "**Features** are characteristics of the texts, variables, such as words appearing in the text or in the title, or any other information about that text that can be used by the model to predict the label. To be understood by the model, features need to be represented as number or boolean, and are organized in mattrix where each rows represent one text and each column a feature. This n-dimensional representation of the features is called the feature space, and the process of extracting and selecting the most pertinent features, is called **feature engineering**.\n",
    "\n",
    "**Process aim:** \n",
    "The aim of this process is to output a file representing the features that will be used to train a model in outputing labels.\n",
    "\n",
    "**Input**: Dataset in JSON or CSV, including one or several unstructured textal fields.\n",
    "\n",
    "Sub-processes:\n",
    "1. Import and prepare dataset\n",
    "2. Feature extraction\n",
    "    * Pre-processing\n",
    "    * Vectorization\n",
    "3. Export features space\n",
    "\n",
    "**Output**: a CSV file representing the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils.extmath import density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import dataset\n",
    "Import the dataset compiling all data acquired through the previous steps (metadata and raw texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the dataset \n",
    "dataset = pd.read_csv('../data/B_engineering/doc_2000_2017_reduced.csv', usecols=['record_id','title','text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset.set_index('record_id',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>record_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>455823</th>\n",
       "      <td>Letter dated 2001/12/06 from the Permanent Rep...</td>\n",
       "      <td>A/56/682–S/2001/1159 United Nations General As...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>694579</th>\n",
       "      <td>Provisional summary record of the 46th meeting...</td>\n",
       "      <td>E/2010/SR.46 United Nations Economic and Socia...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       title  \\\n",
       "record_id                                                      \n",
       "455823     Letter dated 2001/12/06 from the Permanent Rep...   \n",
       "694579     Provisional summary record of the 46th meeting...   \n",
       "\n",
       "                                                        text  \n",
       "record_id                                                     \n",
       "455823     A/56/682–S/2001/1159 United Nations General As...  \n",
       "694579     E/2010/SR.46 United Nations Economic and Socia...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Features Extraction\n",
    "\n",
    "### Preprocessing\n",
    "Pre-processing involves a number of operation that aim at normalizing the text. These include operations such as converting the text in lower case, removing punctuation, removing stop words and tokenization. Tokenization split the text in units called tokens. Tokens can be sentence, world or combination of words (n-grams).\n",
    "\n",
    "Vectorization is the process of converting a text into a vectorized representation of its content. Each tokens becomes a features represented by a numberical value that will be used by machine learning algorithms. There several possibilities to build the vectorized representation of the text.\n",
    "\n",
    "#### Example\n",
    "Corpus of 2 simple texts <code>['The United Nations Development Programme was established in 1965.', 'The United Nations Environment Programme was established in 1972.']</code>\n",
    "\n",
    "1. convert raw text in lower cases => \n",
    "<code>['the united nations development programme was established in 1965.','the united nations environment programme was established in 1972.'\\]</code>\n",
    "2. remove punctuations => <code>['the united nations development programme was established in 1965','the united nations environment programme was established in 1972']</code>\n",
    "3. remove stop words => <code>['united nations development programme established 1965','united nations environment programme established 1972']</code>\n",
    "4. tokenize: split the raw text in smaller units, usually representing words or n-grams.\n",
    "    * world (1-gram) tokenization: => <code>[['united', 'nations', 'development', 'programme', 'established', 1965'], ['united', 'nations', 'environment', 'programme', 'established', 1972']]</code>\n",
    "    * world (2-grams) tokenization: => <code>[['1965','development','development programme','established','established 1965','nations','nations development','programme','programme established','united','united nations'], ['1972','environment','environment programme','established','established 1972','nations','nations environment','programme','programme established','united','united nations']]</code>\n",
    "     \n",
    "### Vectorization\n",
    "\n",
    "Vectorization is the process of converting a text into a vectorized representation of its content. Each tokens becomes a feature represented by a numberical value that will be used by machine learning algorithms to classify texts withing various categories.  There are several possibilities to build the vectorized representation of the text.\n",
    "1. Each feature is represented by a **boolean** value and has the same importance.\n",
    "2. Each feature is represented by its **frequency** each text, without taking into account the length of the text. \n",
    "3. Each feature is representent by its **term frequency-inverse document frequency (TF-IDF)**: normalized the frequency measure so that the frequency of each features in the overall corpus is taken into account. For instance, if 'united nations' appear in all texts, then its importance in caracterizing the text is lowered. \n",
    "\n",
    "#### Document term matrix\n",
    "The result of vectorization is a document-term-matrix, where one row represent a text and one column a feature. The number indicated is either a binary value (0 is not in text, 1 is in text), or the term frequency in the text, or the tf-itf score. \n",
    "The following example show a binary matrix\n",
    "\n",
    "| 1965  | 1972  |  development |environment|  established |  programme  | united |nations| \n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "|1|0|1|0|1|1|1|1|\n",
    "|0|1|0|1|1|1|1|1|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Density\n",
    "In text mining, it is frequent to have a document-term matrix with many zeros, because some features appear only in a few texts. A sparse matrix is a matrix with a high proportion of zeros. On the contrary a dense matrix has a low proportion of zeros. The **density** measures how dense a matrix is. The density of a matrix with few zero_values elements is close to 1. On the other hand, the density of a matrix where the majority of elements are non-zero values is close to 0. This is often the case when analyzing texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features extraction in practice\n",
    "\n",
    "Term frequency and TF-IDF are commonly used to extract features from a corpus of texts. There are more advanced methods.\n",
    "\n",
    "Functions tf_extractor() and tfidf_extractor return:\n",
    "* a vocabulary where each features is an entry: feature_text: feature_number: the vocabulary is not used for machine learning processes. However, it is usefull to map the index number back to the string when investigating which features are the most important. It is structured as a dictionary:\n",
    "<pre>\n",
    "{\n",
    "'1965': 0\n",
    "'1972': 1\n",
    "'development':2\n",
    "...\n",
    "}\n",
    "</pre>\n",
    "* a document-term matrix\n",
    "\n",
    "#### Customization\n",
    "\n",
    "Change the following parameters:\n",
    "* min_df: ignore terms that have a document frequency strictly lower than the given threshold.\n",
    "* ngram_range: you can add more features by selecting the ngram range.\n",
    "* max_features: only consider the top max_features ordered by term frequency across the corpus.\n",
    "* language: you can select another language depending on the language of the corpus, or set it to None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_extractor(dataset, min_df=1, max_df=1.0, ngram_range=(1,1),language='english',max_features=None):\n",
    "    '''\n",
    "    Takes a dataset and several additional parameters\n",
    "    - min_df: will ignore the terms that have a frequency strictly lower than the threshold\n",
    "    - ngram_range: value to extract different n-grams. (1,1) will create token of one word, while (1,2) will create token\n",
    "    one and two words.\n",
    "    - language of the list words that will be excluded (stop words)\n",
    "    Returns a dictionary of features and a document-term frequency matrix. Features are represented by the term frequency\n",
    "    in each text.\n",
    "    '''\n",
    "    vectorizer = CountVectorizer(min_df=min_df, max_df=max_df, ngram_range=ngram_range, stop_words=language,max_features=max_features)\n",
    "    vocabulary = vectorizer.fit(dataset)\n",
    "    matrix = vectorizer.fit_transform(dataset)\n",
    "    \n",
    "    return vocabulary, matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_extractor(dataset, min_df=1, max_df=1.0, ngram_range=(1,1),language='english',max_features=None):\n",
    "    '''\n",
    "    Takes a dataset and several additional parameters\n",
    "    - min_df: will ignore the terms that have a frequency strictly lower than the threshold\n",
    "    - ngram_range: value to extract different n-grams. (1,1) will create token of one word, while (1,2) will create token\n",
    "    one and two words.\n",
    "    - language of the list words that will be excluded (stop words)\n",
    "    Returns a dictionary of features and a document-term ITF matrix. Features are represented by the TF-ITF score\n",
    "    '''\n",
    "    vectorizer = TfidfVectorizer(min_df=min_df, max_df=max_df, ngram_range=ngram_range, stop_words=language,max_features=max_features)\n",
    "    vocabulary = vectorizer.fit(dataset)\n",
    "    matrix = vectorizer.fit_transform(dataset)\n",
    "    return vocabulary, matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_density(dataset, min_df, max_df, ngram_range):\n",
    "    r = []\n",
    "    for n in ngram_range:\n",
    "        for min_val in min_df:\n",
    "            for max_val in max_df:\n",
    "                print(\"n_grams:{}, min_df: {}, max_df: {}\".format(n, min_val, max_val))\n",
    "                tf_idf_vocabulary, tf_idf_matrix = tf_idf_extractor(dataset,min_df=min_val, max_df=max_val, ngram_range=n)\n",
    "                docs  = tf_idf_matrix.shape[0]\n",
    "                features = tf_idf_matrix.shape[1]\n",
    "                pc_nnz = density(tf_idf_matrix)*100\n",
    "                values = [n,min_val,max_val,docs, features,pc_nnz]\n",
    "                r.append(values)\n",
    "    report = pd.DataFrame(r, columns=['n_grams', 'min_df', 'max_df', 'documents', 'features', '% non-zero values'])\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Features from Full Text\n",
    "We will perform feature extraction from the full text using tf_idf_extractor and the default parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the corpus\n",
    "#ft_corpus = dataset['text'].tolist()\n",
    "# Get the vocabulary and the document-term matrix\n",
    "tf_idf_vocabulary_ft, tf_idf_matrix_ft = tf_idf_extractor(dataset['text'], min_df=0.001, max_df=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28376\n"
     ]
    }
   ],
   "source": [
    "# Length of the vocabulary\n",
    "print(len(tf_idf_vocabulary_ft.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix dimension: 113348 documents * 28376 features\n",
      "total number of elements: 3216362848\n",
      "non-zero values: 77713770\n",
      "density: 0.024162003378544187\n",
      "2.42 % of the matrix are non-zero elements\n"
     ]
    }
   ],
   "source": [
    "# Print some information on the sparsity / density of the document-term matrix\n",
    "# Dimension\n",
    "print(\"matrix dimension: {} documents * {} features\".format(tf_idf_matrix_ft.shape[0], tf_idf_matrix_ft.shape[1]))\n",
    "# Total number of elements\n",
    "print(\"total number of elements: {}\".format(tf_idf_matrix_ft.shape[0] * tf_idf_matrix_ft.shape[1]))\n",
    "# Non-zero values\n",
    "print(\"non-zero values: {}\".format(tf_idf_matrix_ft.nnz))\n",
    "# Density\n",
    "print(\"density: {}\".format(density(tf_idf_matrix_ft)))\n",
    "print(\"{0:.2f} % of the matrix are non-zero elements\".format(density(tf_idf_matrix_ft)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Features from Title\n",
    "We will perform features extraction from the title using tf_idf and 3-ngrams tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vocabulary and the document-term matrix\n",
    "tf_idf_vocabulary_title, tf_idf_matrix_title = tf_idf_extractor(dataset['title'], min_df=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix dimension: 113348 documents * 557 features\n",
      "total number of elements: 63134836\n",
      "non-zero values: 1409828\n",
      "density: 0.02233042943201753\n",
      "2.23 % of the matrix are non-zero elements\n"
     ]
    }
   ],
   "source": [
    "# Print some information on the sparsity / density of the document-term matrix\n",
    "# Dimension\n",
    "print(\"matrix dimension: {} documents * {} features\".format(tf_idf_matrix_title.shape[0], tf_idf_matrix_title.shape[1]))\n",
    "# Total number of elements\n",
    "print(\"total number of elements: {}\".format(tf_idf_matrix_title.shape[0] * tf_idf_matrix_title.shape[1]))\n",
    "# Non-zero values\n",
    "print(\"non-zero values: {}\".format(tf_idf_matrix_title.nnz))\n",
    "# Density\n",
    "print(\"density: {}\".format(density(tf_idf_matrix_title)))\n",
    "print(\"{0:.2f} % of the matrix are non-zero elements\".format(density(tf_idf_matrix_title)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_features = pd.DataFrame(tf_idf_matrix_ft.toarray(),columns=tf_idf_vocabulary_ft.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_features = ft_features.set_index(dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_features = pd.DataFrame(tf_idf_matrix_title.toarray(),columns=tf_idf_vocabulary_title.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_features = title_features.set_index(dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft_features.to_csv('../data/C_learning/doc_2000_20007_ft_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_features.to_csv('../data/C_learning/doc_2000_20007_title_features.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
