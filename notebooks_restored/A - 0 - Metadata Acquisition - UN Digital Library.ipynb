{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree, objectify\n",
    "import urllib3\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_prefixes(xml_root):\n",
    "    '''\n",
    "    This function takes root as argument and clean it from any prefix, before returning it.\n",
    "    '''\n",
    "    for e in xml_root.getiterator():\n",
    "        if hasattr(e.tag, 'find'):\n",
    "            i = e.tag.find('}')\n",
    "            if i >= 0:\n",
    "                e.tag = e.tag[i+1:]\n",
    "    return xml_root\n",
    "\n",
    "def get_url_to_parse(base_url, start=1, total_results=5743, results_per_page=200):\n",
    "    '''\n",
    "    From a base url, query the UN digital library, and get urls for all result pages.\n",
    "    Base url can contain all parameters of the query, but rg and jrec. results per page is set to 200\n",
    "    because it is the maximum number of records a regular user can download.\n",
    "    '''\n",
    "    url = base_url + '&rg=' + str(results_per_page) + '&jrec=' + str(start) + '&of=xm'\n",
    "    response = urllib3.PoolManager().request('GET', url) \n",
    "    total_results = int(re.search(r\":\\s(\\d+)\\s-->\",str(response.data)).group(1))\n",
    "    print(total_results)\n",
    "    url_list = []\n",
    "    while start <= total_results:\n",
    "        url = base_url + '&rg=' + str(results_per_page) + '&jrec=' + str(start) + '&of=xm'\n",
    "        start += results_per_page\n",
    "        url_list.append(url)\n",
    "    return url_list\n",
    "\n",
    "def url_to_xml(url):\n",
    "    '''\n",
    "    Given a list of urls leading to xml documents, will query the url, and return a list of related XML trees.\n",
    "    '''\n",
    "    print(url)\n",
    "    with urllib3.PoolManager(num_pools=40) as http:\n",
    "        try:\n",
    "            r = http.request('GET', url, preload_content=False)\n",
    "        except:\n",
    "            logger.exception('Error')\n",
    "    try:\n",
    "        tree = objectify.parse(r)\n",
    "    except:\n",
    "        logger.exception('Error')\n",
    "    return tree\n",
    "\n",
    "def merge_xml(trees,root,child):\n",
    "    '''\n",
    "    Given a list of xml trees will merge them into a single XML document.\n",
    "    '''\n",
    "    collection = etree.Element(root)\n",
    "    for tree in trees:\n",
    "        root = clean_prefixes(tree.getroot())\n",
    "        childNodeList = root.findall(child)\n",
    "        for node in childNodeList: \n",
    "            collection.append(node)\n",
    "    document = etree.ElementTree(collection)\n",
    "    return document\n",
    "\n",
    "def save_marcxml(url,save_path):\n",
    "    pages_list = get_url_to_parse(url)\n",
    "    xml_trees = [url_to_xml(page) for page in pages_list]\n",
    "    merged_xml = merge_xml(xml_trees,'collection','record')\n",
    "    print(len(merged_xml.findall('record')))\n",
    "    merged_xml.write(path)\n",
    "\n",
    "def get_metadata(r):\n",
    "    \"\"\"\n",
    "    Takes a MARC XML record. For each metadata, the function will call annother get_element, which will return\n",
    "    'None', a string (one value found), or a list of string (multiple values found)\n",
    "    \"\"\"\n",
    "    # Each record will be a dictionary and include\n",
    "    metadata =  {\n",
    "        # Standard MARC fields\n",
    "        \"record_id\": get_md_value(r,\"controlfield[@tag='001']\"),\n",
    "        # The title field concatenate 3 subfields (a, b, c)\n",
    "        \"title\": get_md_value(\n",
    "            r,\"datafield[@tag='245']/subfield[@code='a']\") + \" \" + get_md_value(\n",
    "            r,\"datafield[@tag='245']/subfield[@code='b']\") + \" \" + get_md_value(\n",
    "            r,\"datafield[@tag='245']/subfield[@code='c']\"),\n",
    "        \"subject-topics\": get_md_value(r,\"datafield[@tag='650']/subfield[@code='a']\"),\n",
    "        \"subject-geo\": get_md_value(r,\"datafield[@tag='651']/subfield[@code='a']\"),\n",
    "        \"publication_date\": get_md_value(r,\"datafield[@tag='269']/subfield[@code='a']\"),\n",
    "        #\"subject-corporates\": get_md_value(r,\"datafield[@tag='610']/subfield[@code='a']\"),\n",
    "        #\"subject-titles\": get_md_value(r,\"datafield[@tag='630']/subfield[@code='a']\"),\n",
    "        # Local MARC fields in used for UN documents\n",
    "        \"symbol\": get_md_value(r,\"datafield[@tag='191']/subfield[@code='a']\"),\n",
    "        \"body\": get_md_value(r,\"datafield[@tag='191']/subfield[@code='b']\"),\n",
    "        #\"session\": get_md_value(r,\"datafield[@tag='191']/subfield[@code='c']\"),\n",
    "    }\n",
    "    # Call the function get_files_info. This function return annother dictionary{description: url}\n",
    "    files_info = get_files_md(r)\n",
    "    # Merge the initial metadata dictionary with the dictionary containing the files information\n",
    "    metadata = {**metadata,**files_info}\n",
    "    return metadata\n",
    "\n",
    "def get_files_md(r):\n",
    "    '''\n",
    "    This function get the URL from the xml records as well as the description. It forms a dictionary,\n",
    "    where the description is the key and the url the value.\n",
    "    '''\n",
    "    files = {}\n",
    "    MARC_856 = r.findall(\"datafield[@tag='856'][@ind1='4']\")\n",
    "    if len(MARC_856) > 0:\n",
    "        for item in MARC_856:\n",
    "            description = \"url-\" + get_md_value(item, \"subfield[@code='y']\")\n",
    "            # Deals with some variation of encoding in the name of languages (Specific to UN)\n",
    "            description = description.replace('ñ','ñ').replace('ç','ç')\n",
    "            url = get_md_value(item, \"subfield[@code='u']\")\n",
    "            files[description] = url\n",
    "    return files\n",
    "\n",
    "def get_md_value(marc_xml_record,query):\n",
    "    \"\"\"\n",
    "    Takes 2 arguments: a MARC XML record and a query (XPath) that identifies an XML element. \n",
    "    Queries the record to identify the targeted element. If no element is found return 'None', if one element is found returns a string, if more than one\n",
    "    elements are found returns a list of strings.\n",
    "    \"\"\"\n",
    "    # Parse the record to get all matchin gelements\n",
    "    xml_element = marc_xml_record.findall(query)\n",
    "    # Process xml_element according to its length to return either a string, or a list of strings.\n",
    "    if len(xml_element)>1:\n",
    "        values = []\n",
    "        i = 0\n",
    "        for item in xml_element:\n",
    "            element = values.append(xml_element[i].text)\n",
    "            i +=1\n",
    "        return values # multiple values, returns a list of strings\n",
    "    elif len(xml_element) == 1:\n",
    "        return xml_element[0].text # one value, retunrns a string\n",
    "    else:\n",
    "        return \"\" # no value, returns an empty string.\n",
    "\n",
    "def convert_marxml_to_csv(input_path,output_path):\n",
    "    # Set a variable with parameters on how the parser should behave.\n",
    "    parser = etree.XMLParser(encoding='utf-8')\n",
    "    # Import xml file as an xml etree\n",
    "    tree = etree.parse(input_path,parser)\n",
    "    # Remove prefixes in XML elements\n",
    "    root = clean_prefixes(tree.getroot())\n",
    "    # Get all <records> element in XML MARC\n",
    "    xml_records = root.findall(\"record\")\n",
    "    dictionary_records = [get_metadata(r) for r in xml_records]\n",
    "    md_dataset = (pd.DataFrame(dictionary_records))\n",
    "    md_dataset.to_csv(output_path)\n",
    "\n",
    "def get_csv(year):\n",
    "    df = (pd.read_csv('data/acquisition/metadata/' + year +'.csv')\n",
    "          .set_index('record_id'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://digitallibrary.un.org/search?ln=en&p=year%3A2000-%3E2017+AND+(collection%3A%22General+Assembly%22+OR+collection%3A%22Security+Council%22+OR+collection%3A%22Economic+and+social+council%22)+AND+collection%3A%22Documents+and+Publications%22'\n",
    "path= 'data/acquisition/metadata/doc_2000_2017.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "151804\n"
     ]
    }
   ],
   "source": [
    "save_marcxml(url,path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
