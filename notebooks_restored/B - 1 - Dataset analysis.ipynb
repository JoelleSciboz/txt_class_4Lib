{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B - 1 - Dataset analysis\n",
    "\n",
    "## Description\n",
    "\n",
    "The dataset influence the performance the machine learning model. The dataset is composed of identifiers, features and labels:\n",
    "* **Features** are characteristics of the texts, variables used to classify, such as words appearing in the text or in the title, or any other information about that text that can be use by the model to predict the label. To be understood by the model, features need to be represented as number or boolean, and are organized in mattrix where each rows represent one text and each column a feature. This n-dimensional representation of the features is called the feature space, and the process of extracting and selecting the most pertinent features, is called **feature engineering**.\n",
    "\n",
    "* **Labels** are used during the training phase. They are the categories or class assigned to each text and that the learning algorithm will need to predict. They can represent a topic, a person, a type of material, etc. They are typically stored in a matrix of boolean values where each row represent a text and each column a label, this matrix is called the **label space**. The **label set** is the ensemble of all possible labels. \n",
    "\n",
    "A large feature space can influence positively the accuracy of the predictions, but at the same time, the more features, the more computationnally expensive it will be to train the model, especially when trying to output several labels by text. Because the distribution of label is often inequal (some labels are used more than other), the larger the labelset is, the more data (features, text) are needed, and the more likely it is to encounter computational limitations during the process. The difficulty arising from large dimensional features and label spaces is called **the curse of dimensionality**. \n",
    "\n",
    "Analyzing the dataset before training the model is important. It can lead to better features engineering and labelset reduction, and training process.\n",
    "\n",
    "**Process aim:**\n",
    "Gathering insight on the dataset to identify potential issues and mitigate them when engineering the features and building the label space.\n",
    "\n",
    "**Input:** A csv files\n",
    "**Sub-processes**:\n",
    "\n",
    "\n",
    "**Output:** a CSV file, statistical reports and graphics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.utils.extmath import density\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset \n",
    "dataset = pd.read_csv('data/A_input_data/metadata/output/doc_2000_2017_txt_clean.csv', index_col='record_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels\n",
    "In this example, we aim at outputting the subject of text document by learning from documents that have already been subject indexed. Subject information are contain in the following column:\n",
    "* subject_topic: all topical subjects terms\n",
    "* subject_primary: a subeset of subject topics that contain the most important subjects of each document\n",
    "* subject_geo: geographical subjects terms \n",
    "\n",
    "Each of these fields, or a combination of them can compose our label set. The following information about the labelset will help us to take informed decision to tune our learning process.\n",
    "* number of label by record, and evolution of this number in time,\n",
    "* usage of each label in the labelset, \n",
    "* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels by documents\n",
    "#### Create new label columns \n",
    "Geographic terms are used less often than topical subjects, in order to take all types of subjects in consideration during the learning process, we will group these terms togethe in new columns:\n",
    "To allow further analysis, we start by creating new columns that could be used as different labelset.\n",
    "* all_subjects: groups subject-topic and subject-geo\n",
    "* subjects_primary-geo: groups subject-primary and subject-geo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a columns all_subjects concatenating topics and geographical terms\n",
    "dataset['subjects_all'] = (dataset\n",
    "                           .apply(lambda x:'%s||%s' % (x['subjects_topics'],x['subjects_geo']),axis=1)\n",
    "                           .apply(lambda x: x.replace('nan||','')) # clean nan strings\n",
    "                           .apply(lambda x: x.replace('||nan','')) # clean nan strings\n",
    "                          )\n",
    "dataset['subjects_all'] = dataset.subjects_all.replace('nan',np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a columns all_subjects concatenating topics-primary and geographical terms\n",
    "dataset['subjects_primary_geo'] = (dataset\n",
    "                                   .apply(lambda x:'%s||%s' % (x['subjects_primary'],x['subjects_geo']),axis=1)\n",
    "                                   .apply(lambda x: x.replace('nan||','')) # clean nan strings\n",
    "                                   .apply(lambda x: x.replace('||nan','')) # clean nan strings\n",
    ")\n",
    "dataset['subjects_primary_geo'] = dataset.subjects_primary_geo.replace('nan',np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each columns that could be used as labelset, we can count the number of label per record. This will give us a primary idea of indexing practices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_multiple_values(values,separator):\n",
    "    if isinstance(values,str):\n",
    "        return len(values.split(separator))\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column that count the number of subjects assigned to each document\n",
    "dataset['subjects_topics_count'] = (dataset['subjects_topics']\n",
    "                                    .apply(lambda x: count_multiple_values(x,'||'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column that count the number of subjects assigned to each document\n",
    "dataset['subjects_geo_count'] = (dataset['subjects_geo']\n",
    "                                 .apply(lambda x: count_multiple_values(x,'||'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column that count the number of subjects assigned to each document\n",
    "dataset['subjects_primary_count'] = (dataset['subjects_primary']\n",
    "                                 .apply(lambda x: count_multiple_values(x,'||'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column that count the number of subjects assigned to each document\n",
    "dataset['subjects_all_count'] = (dataset['subjects_all']\n",
    "                                 .apply(lambda x: count_multiple_values(x,'||'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column that count the number of subjects assigned to each document\n",
    "dataset['subjects_primary_geo_count'] = (dataset['subjects_primary_geo']\n",
    "                                 .apply(lambda x: count_multiple_values(x,'||'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labelset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sets(dataset, field, values,label_field,separator):\n",
    "    sets = []\n",
    "    subsets = []\n",
    "    dataset = (dataset[dataset[label_field].notnull()]\n",
    "              .reset_index()\n",
    "            )\n",
    "    for v in values:\n",
    "        subset = (dataset[dataset[field] == v].copy().reset_index(drop=True))\n",
    "        labelset = get_binary_labels(subset,label_field,separator)\n",
    "        sets.append([label_field, str(v), subset, labelset])\n",
    "        subsets.append(subset)\n",
    "        if v != values[0]:\n",
    "            cumulset = pd.concat(subsets)\n",
    "            labelset = get_binary_labels(cumulset,label_field,separator)\n",
    "            sets.append([label_field, str(values[0]) + '-'+ str(v), cumulset,labelset])\n",
    "    return sets\n",
    "\n",
    "def get_binary_labels(dataset,label_field,separator=','):\n",
    "    '''\n",
    "    Takes a pandas dataset, the name of the column containing labels, and separator (i.e. multivalue):\n",
    "    - transform the column representing the label to a list\n",
    "    - split it to a list of labels for each row\n",
    "    - transform the list of textual labels to a binary sparse matrix\n",
    "    - return the binary sparse matrix representing labels as well as the list of associated textual labels.\n",
    "    '''\n",
    "    labels = dataset[label_field].tolist()\n",
    "    labels = [l.split(separator) for l in labels]\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    binary_labels = mlb.fit_transform(labels)\n",
    "    labels = mlb.classes_\n",
    "    labelset = pd.DataFrame(binary_labels, columns=labels)\n",
    "    return labelset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_report(dataset, columns, name='all',groups=None):\n",
    "    reports = {name: dataset[columns].describe()}\n",
    "    if groups is not None:\n",
    "        for i in range(len(groups)):\n",
    "            reports[groups[i]] = dataset.groupby(groups[i])[columns].describe()            \n",
    "    return reports\n",
    "\n",
    "def create_report(dataset):\n",
    "    labelset = dataset[3].copy().T\n",
    "    labelset['cardinality'] = labelset.sum(axis=1)\n",
    "    reports = {dataset[0] + '_' + dataset[1]: \n",
    "               {'nr_of_records': dataset[2][dataset[0] + '_count'].describe(),\n",
    "                'label_cardinality':labelset['cardinality'].describe(),\n",
    "                'label_ranking': labelset.sort_values('cardinality',ascending=False)\n",
    "                }\n",
    "              }\n",
    "    return reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "field = 'year'\n",
    "values = [2000,2001,2002,2003,2004,2005,2006,2007,2008,2009,2010,2011,2012,2013,2014,2015,2016]\n",
    "label_field = 'subjects_all'\n",
    "separator = '||'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = create_sets(dataset, field, values,label_field,separator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjects_all_reports = [create_report(s) for s in subsets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_describe_reports(df, columns, name='all',groups=None):\n",
    "    reports = {name: df[columns].describe()}\n",
    "    if groups is not None:\n",
    "        for i in range(len(groups)):\n",
    "            reports[groups[i]] = df.groupby(groups[i])[columns].describe()            \n",
    "    return reports\n",
    "\n",
    "def save_reports(reports,file_path,name):\n",
    "    # Create a Pandas Excel writer using XlsxWriter as the engine.\n",
    "    writer = pd.ExcelWriter(file_path + name + '.xlsx', engine='xlsxwriter')\n",
    "    # Write each dataframe to a different worksheet.\n",
    "    for k,v in reports.items():\n",
    "        v.to_excel(writer, sheet_name=k)\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label by documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report parameter\n",
    "report_columns = ['subjects_topics_count',\n",
    "       'subjects_geo_count', 'subjects_primary_count', 'subjects_all_count',\n",
    "       'subjects_primary_geo_count']\n",
    "report_groups = ['main_body','year']\n",
    "report_name = '650_651'\n",
    "path = 'reports/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the report\n",
    "label_by_documents = create_describe_reports(dataset,report_columns, report_name, report_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_by_documents['650_651']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_by_documents['main_body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_by_documents['year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the report\n",
    "save_reports(label_by_documents, path, report_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
