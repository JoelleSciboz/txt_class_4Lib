{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B - 3 - Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "**Process aim:** \n",
    "The aim of this process is to output a file representing the features that will be used to train a model in outputing labels.\n",
    "\n",
    "**Input**: Dataset in JSON or CSV, including one or several unstructured textal fields.\n",
    "\n",
    "Sub-processes:\n",
    "1. Import and prepare dataset\n",
    "2. Feature extraction\n",
    "    * Pre-processing\n",
    "    * Vectorization\n",
    "3. Export features space\n",
    "\n",
    "**Output**: a CSV file representing the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils.extmath import density"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import dataset\n",
    "Import the dataset compiling all data acquired through the previous steps (metadata and raw texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import the dataset \n",
    "dataset = pd.read_csv('data/pre-processing/dataset_from_MARC_text_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Features Extraction\n",
    "\n",
    "### Preprocessing\n",
    "Pre-processing involves a number of operation that aim at normalizing the text. These include operations such as converting the text in lower case, removing punctuation, removing stop words and tokenization. Tokenization split the text in units called tokens. Tokens can be sentence, world or combination of words (n-grams).\n",
    "\n",
    "Vectorization is the process of converting a text into a vectorized representation of its content. Each tokens becomes a features represented by a numberical value that will be used by machine learning algorithms. There several possibilities to build the vectorized representation of the text.\n",
    "\n",
    "#### Example\n",
    "Corpus of 2 simple texts <code>['The United Nations Development Programme was established in 1965.', 'The United Nations Environment Programme was established in 1972.']</code>\n",
    "\n",
    "1. convert raw text in lower cases => \n",
    "<code>['the united nations development programme was established in 1965.','the united nations environment programme was established in 1972.'\\]</code>\n",
    "2. remove punctuations => <code>['the united nations development programme was established in 1965','the united nations environment programme was established in 1972']</code>\n",
    "3. remove stop words => <code>['united nations development programme established 1965','united nations environment programme established 1972']</code>\n",
    "4. tokenize: split the raw text in smaller units, usually representing words or n-grams.\n",
    "    * world (1-gram) tokenization: => <code>[['united', 'nations', 'development', 'programme', 'established', 1965'], ['united', 'nations', 'environment', 'programme', 'established', 1972']]</code>\n",
    "    * world (2-grams) tokenization: => <code>[['1965','development','development programme','established','established 1965','nations','nations development','programme','programme established','united','united nations'], ['1972','environment','environment programme','established','established 1972','nations','nations environment','programme','programme established','united','united nations']]</code>\n",
    "     \n",
    "### Vectorization\n",
    "\n",
    "Vectorization is the process of converting a text into a vectorized representation of its content. Each tokens becomes a feature represented by a numberical value that will be used by machine learning algorithms to classify texts withing various categories.  There are several possibilities to build the vectorized representation of the text.\n",
    "1. Each feature is represented by a **boolean** value and has the same importance.\n",
    "2. Each feature is represented by its **frequency** each text, without taking into account the length of the text. \n",
    "3. Each feature is representent by its **term frequency-inverse document frequency (TF-IDF)**: normalized the frequency measure so that the frequency of each features in the overall corpus is taken into account. For instance, if 'united nations' appear in all texts, then its importance in caracterizing the text is lowered. \n",
    "\n",
    "#### Document term matrix\n",
    "The result of vectorization is a document-term-matrix, where one row represent a text and one column a feature. The number indicated is either a binary value (0 is not in text, 1 is in text), or the term frequency in the text, or the tf-itf score. \n",
    "The following example show a binary matrix\n",
    "\n",
    "| 1965  | 1972  |  development |environment|  established |  programme  | united |nations| \n",
    "|---|---|---|---|---|---|---|---|---|\n",
    "|1|0|1|0|1|1|1|1|\n",
    "|0|1|0|1|1|1|1|1|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Density\n",
    "In text mining, it is frequent to have a document-term matrix with many zeros, because some features appear only in a few texts. A sparse matrix is a matrix with a high proportion of zeros. On the contrary a dense matrix has a low proportion of zeros. The **density** measures how dense a matrix is. The density of a matrix with few zero_values elements is close to 1. On the other hand, the density of a matrix where the majority of elements are non-zero values is close to 0. This is often the case when analyzing texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features extraction in practice\n",
    "\n",
    "Term frequency and TF-IDF are commonly used to extract features from a corpus of texts. There are more advanced methods.\n",
    "\n",
    "Functions tf_extractor() and tfidf_extractor return:\n",
    "* a vocabulary where each features is an entry: feature_text: feature_number: the vocabulary is not used for machine learning processes. However, it is usefull to map the index number back to the string when investigating which features are the most important. It is structured as a dictionary:\n",
    "<pre>\n",
    "{\n",
    "'1965': 0\n",
    "'1972': 1\n",
    "'development':2\n",
    "...\n",
    "}\n",
    "</pre>\n",
    "* a document-term matrix\n",
    "\n",
    "#### Customization\n",
    "\n",
    "Change the following parameters:\n",
    "* min_df: ignore terms that have a document frequency strictly lower than the given threshold.\n",
    "* ngram_range: you can add more features by selecting the ngram range.\n",
    "* max_features: only consider the top max_features ordered by term frequency across the corpus.\n",
    "* language: you can select another language depending on the language of the corpus, or set it to None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_extractor(corpus, min_df=1, ngram_range=(1,1),language=None,max_features=None):\n",
    "    '''\n",
    "    Takes a corpus and several additional parameters\n",
    "    - min_df: will ignore the terms that have a frequency strictly lower than the threshold\n",
    "    - ngram_range: value to extract different n-grams. (1,1) will create token of one word, while (1,2) will create token\n",
    "    one and two words.\n",
    "    - language of the list words that will be excluded (stop words)\n",
    "    Returns a dictionary of features and a document-term frequency matrix. Features are represented by the term frequency\n",
    "    in each text.\n",
    "    '''\n",
    "    vectorizer = CountVectorizer(min_df=min_df, ngram_range=ngram_range, stop_words=language,max_features=max_features)\n",
    "    vocabulary = vectorizer.fit(corpus)\n",
    "    matrix = vectorizer.fit_transform(corpus)\n",
    "    \n",
    "    return vocabulary, matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf_extractor(corpus, min_df=1, ngram_range=(1,1),language='english',max_features=None):\n",
    "    '''\n",
    "    Takes a corpus and several additional parameters\n",
    "    - min_df: will ignore the terms that have a frequency strictly lower than the threshold\n",
    "    - ngram_range: value to extract different n-grams. (1,1) will create token of one word, while (1,2) will create token\n",
    "    one and two words.\n",
    "    - language of the list words that will be excluded (stop words)\n",
    "    Returns a dictionary of features and a document-term ITF matrix. Features are represented by the TF-ITF score\n",
    "    '''\n",
    "    vectorizer = TfidfVectorizer(min_df=min_df, ngram_range=ngram_range, stop_words=language,max_features=max_features)\n",
    "    vocabulary = vectorizer.fit(corpus)\n",
    "    matrix = vectorizer.fit_transform(corpus)\n",
    "    return vocabulary, matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Features from Full Text\n",
    "We will perform feature extraction from the full text using tf_idf_extractor and the default parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit\n",
    "# Create the corpus\n",
    "ft_corpus = dataset['text'].tolist()\n",
    "# Get the vocabulary and the document-term matrix\n",
    "tf_idf_vocabulary_ft, tf_idf_matrix_ft = tf_idf_extractor(ft_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75378\n"
     ]
    }
   ],
   "source": [
    "# Length of the vocabulary\n",
    "print(len(tf_idf_vocabulary_ft.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix dimension: 2553 documents * 75378 features\n",
      "total number of elements: 192440034\n",
      "non-zero values: 2000955\n",
      "density: 0.01039781046806508\n",
      "1.04 % of the matrix are non-zero elements\n"
     ]
    }
   ],
   "source": [
    "# Print some information on the sparsity / density of the document-term matrix\n",
    "# Dimension\n",
    "print(\"matrix dimension: {} documents * {} features\".format(tf_idf_matrix_ft.shape[0], tf_idf_matrix_ft.shape[1]))\n",
    "# Total number of elements\n",
    "print(\"total number of elements: {}\".format(tf_idf_matrix_ft.shape[0] * tf_idf_matrix_ft.shape[1]))\n",
    "# Non-zero values\n",
    "print(\"non-zero values: {}\".format(tf_idf_matrix_ft.nnz))\n",
    "# Density\n",
    "print(\"density: {}\".format(density(tf_idf_matrix_ft)))\n",
    "print(\"{0:.2f} % of the matrix are non-zero elements\".format(density(tf_idf_matrix_ft)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Features from Title\n",
    "We will perform features extraction from the title using tf_idf and 3-ngrams tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the corpus\n",
    "title_corpus = dataset['title'].tolist()\n",
    "# Get the vocabulary and the document-term matrix\n",
    "tf_idf_vocabulary_title, tf_idf_matrix_title = tf_idf_extractor(title_corpus,ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20855\n"
     ]
    }
   ],
   "source": [
    "# Length of the vocabulary\n",
    "print(len(tf_idf_vocabulary_title.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix dimension: 2553 documents * 20855 features\n",
      "total number of elements: 53242815\n",
      "non-zero values: 125699\n",
      "density: 0.0023608631512064115\n",
      "0.24 % of the matrix are non-zero elements\n"
     ]
    }
   ],
   "source": [
    "# Print some information on the sparsity / density of the document-term matrix\n",
    "# Dimension\n",
    "print(\"matrix dimension: {} documents * {} features\".format(tf_idf_matrix_title.shape[0], tf_idf_matrix_title.shape[1]))\n",
    "# Total number of elements\n",
    "print(\"total number of elements: {}\".format(tf_idf_matrix_title.shape[0] * tf_idf_matrix_title.shape[1]))\n",
    "# Non-zero values\n",
    "print(\"non-zero values: {}\".format(tf_idf_matrix_title.nnz))\n",
    "# Density\n",
    "print(\"density: {}\".format(density(tf_idf_matrix_title)))\n",
    "print(\"{0:.2f} % of the matrix are non-zero elements\".format(density(tf_idf_matrix_title)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save full text features\n",
    "ft_features = pd.DataFrame(tf_idf_matrix_ft.toarray(),columns=tf_idf_vocabulary_ft.get_feature_names())\n",
    "#ft_features['nnz-ft'] = [tf_idf_matrix_ft[i].nnz for i in range( 0, tf_idf_matrix_ft.shape[0])]\n",
    "ft_features.to_csv('data/input spaces/ft_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save title features\n",
    "title_features = pd.DataFrame(tf_idf_matrix_title.toarray(),columns=tf_idf_vocabulary_title.get_feature_names())\n",
    "title_features.to_csv('data/input spaces/title_features.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
