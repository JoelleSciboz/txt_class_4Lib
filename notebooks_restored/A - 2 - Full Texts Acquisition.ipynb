{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A - 2 - Full Texts Acquisition\n",
    "\n",
    "## Description\n",
    "**Process aim:**\n",
    "This process aims at adding a field containing the full text of the resources to the metadata dataframe.\n",
    "\n",
    "**Input:** A csv files containing metadata including at least one columns with URLs\n",
    "\n",
    "**Sub-processes**:\n",
    "1. Import metadata\n",
    "2. Get and save PDF files\n",
    "3. Extract full texts from PDF files\n",
    "4. Add full text to dataset and save\n",
    "\n",
    "**Output:** a CSV file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import textract\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the name of the fields to keep. These might be either \n",
    "* identifiers (ie. record_id)\n",
    "* target labels: field that are intended to be predicted in other world automatically generated (i.e. subjects_geo, subjects_topics, etc)\n",
    "* url of the resource in English (i.e. url_English)\n",
    "* features: field that includes characteristics of the text that will be used to predict the labels (i.e. title)\n",
    "* any other field that you want to keep to analyze your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns of the dataset to keep\n",
    "columns = ['record_id','body', 'date', 'session', 'subjects_geo','subjects_primary', 'subjects_topics', 'symbol', \n",
    "           'title', 'type','url_English']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and create a Pandas dataframe\n",
    "dataset = (pd.read_csv('data/0_input_data/metadata/input/doc_2000_2017.csv',usecols=columns, index_col='record_id', dtype='str'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get and save PDF files\n",
    "We need the full text of the resources described in the MARC XML that will be used later to infer some metadata. In this case we will focus on English texts only.\n",
    "\n",
    "For this step start by creating a list containing for each record that has an English url, the record id, and the url. We then use the function save_files() to get the files using the url and save them in the the folder data/acquisition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_files(files_list, save_path, file_extension):\n",
    "    '''\n",
    "    Takes a list of of lists (record_id and url), the path of the location where the files\n",
    "    will be saved, and the extension of the file type. Get the files through htpp requests\n",
    "    and save them. Returns a list of record_id, corresponding to files that could not be \n",
    "    saved.\n",
    "    '''\n",
    "    errors = []\n",
    "    for item in files_list:\n",
    "        save_as = save_path + str(item[0]) + file_extension\n",
    "        file_url = item[1]\n",
    "        response = requests.get(file_url)\n",
    "        if response.status_code == requests.codes.ok:\n",
    "            with open(save_as, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "        else:\n",
    "            errors.append(item)\n",
    "    return errors\n",
    "\n",
    "def last_saved_file(record_id,file_list):\n",
    "    '''\n",
    "    Print the index of corresponding to the record_id in file_list\n",
    "    To use if save_files stops in order to restart the downloads where it stoped.\n",
    "    '''\n",
    "    i = 0\n",
    "    for item in file_list:\n",
    "        if record_id in item:\n",
    "            print('{} : {}'.format(i,item))\n",
    "        i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of record_id and url for all record that have an url\n",
    "en_list = (dataset.reset_index()[['record_id', 'url_English']]\n",
    "           .dropna() # filter out if non values\n",
    "           .values.tolist())\n",
    "# Output the length of the list\n",
    "len(en_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the files and save them in pdf\n",
    "save_files(en_list, 'data/A_input_data/files/', '.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the script stops running before the list is completed, get the latest saved file, use last_saved_file to get the index number and restart at index number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# last_record_id = # past the record_id of the latest file saved\n",
    "# last_saved_file(last_record_id,en_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart at index, replace *** by the index number\n",
    "# save_files(en_list[***:], 'data/A_input_data/files/', '.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract full text from PDF files\n",
    "Using the same files list we then use the convert_to_pdf_function, to get the content of the PDFs, convert it to a string of text, and store this as a third column in the initial list. Note that if a page cannot be processed, then it will be skipped altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pdf_to_text(files_list, path):\n",
    "    '''\n",
    "    Takes a list of list, and a path to files in pdf. Read each files and convert to text.\n",
    "    Append the resulting texts to the initial list and return the list.\n",
    "    '''\n",
    "    new_list = []\n",
    "    i = 1\n",
    "    for file in files_list:\n",
    "        file_path = path + str(file[0]) + '.pdf'\n",
    "        full_text = \"\"\n",
    "        try: \n",
    "            full_text = textract.process(file_path)\n",
    "            full_text = full_text.decode() # convert unicode bytes\n",
    "        except:\n",
    "            logger.exception(\"record {}: could not convert pdf to text\".format(file[0]))\n",
    "        file.append(full_text)    \n",
    "        new_list.append(file)\n",
    "        i +=1\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column with the full text of the pdf\n",
    "en_list = convert_pdf_to_text(en_list,'data/A_input_data/files/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Add full texts to the metadata and save the output\n",
    "To finish, we create a new dataframe with only the record_id and the full text. As they have the same index, the record-id, we can join it to the metadata dataframe and easily save the result as a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe withg the record_id and the full text\n",
    "full_text = (pd.DataFrame(en_list, columns=['record_id','url','text'])\n",
    "             .drop('url', axis=1)\n",
    "             .set_index('record_id')\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the result to the metadta dataset\n",
    "dataset = dataset.join(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the result\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the content of the dataset in data/pre-processing/\n",
    "dataset.to_csv('data/A_input_data/metadata/output/doc_2000_2017.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
